# Streaming Upload Implementation Summary

## Problem Solved

Railway has **ephemeral filesystem** and **limited memory**. The bot was running out of memory during long recording sessions because all audio was buffered in RAM until the recording stopped.

For a 3-hour session with 4 users:
- **Old approach**: ~8GB of PCM audio in memory
- **New approach**: ~5-10MB max (only current segments in memory)

## Solution Architecture

### Stream-to-Cloud Pattern

Audio segments are uploaded to Vercel Blob Storage **immediately as they complete** (after silence gaps), then deleted from memory and disk.

```
Audio Stream â†’ PCM Buffer â†’ Segment Complete (silence gap)
    â†“
Write to temp WAV file
    â†“
Upload to Blob Storage (via API)
    â†“
Delete temp file
    â†“
Clear buffers from memory
    â†“
Store only blob URL + metadata
```

## Implementation Details

### 1. New API Endpoints (Required)

See `STREAMING_UPLOAD_API_SPEC.md` for complete specifications.

**Three new endpoints:**
- `POST /api/recordings/init-live` - Initialize recording at start
- `POST /api/recordings/{recordingId}/segment-upload-url` - Get upload URL for each segment
- `POST /api/recordings/{recordingId}/finalize` - Complete recording with all blob URLs

### 2. Bot Changes

#### File: `src/types/recording-api.ts`
- Added `RecordingInitLiveRequest/Response` types
- Added `SegmentUploadUrlRequest/Response` types
- Added `RecordingFinalizeRequest/Response` types
- Added `RecordingSegmentWithBlob` type
- Added `'live'` to `RecordingStatus` enum

#### File: `src/services/upload/RecordingUploadService.ts`
- **New method**: `initLiveRecording()` - Initialize recording session
- **New method**: `uploadSegmentImmediately()` - Upload single segment on-the-fly
- **New method**: `finalizeRecording()` - Finalize with all uploaded segments

#### File: `src/services/recording/BasicRecordingService.ts`

**Modified `AudioSegment` interface:**
```typescript
interface AudioSegment {
  // ... existing fields
  filePath?: string;  // Blob URL after upload
}
```

**Modified `SessionMetadata` interface:**
```typescript
interface SessionMetadata {
  // ... existing fields
  outputDirectory: string;  // Temp directory
  recordingId?: string;  // From API
  uploadedSegments: RecordingSegmentWithBlob[];  // Track uploads
}
```

**New method**: `writeAndUploadSegment()`
- Converts PCM buffers to WAV
- Uploads to blob storage
- Deletes local file
- Stores blob URL in `uploadedSegments[]`

**Modified `startRecording()`**:
- Added `guildId` and `userId` parameters
- Creates temporary output directory (`/tmp/recordings/{sessionId}`)
- Calls `initLiveRecording()` to get recordingId
- Initializes `uploadedSegments` array

**Modified silence gap detection** (line ~460):
- When segment completes, copies buffer chunks
- **Immediately clears original buffers** (frees memory)
- Calls `writeAndUploadSegment()` asynchronously
- Continues recording without blocking

**Modified `stopAndExport()`**:
- Uploads any final segments
- If `recordingId` exists â†’ calls `finalizeRecording()`
- Otherwise â†’ falls back to batch export
- Cleans up temp directory after streaming upload

#### File: `src/services/recording/RecordingManager.ts`
- Updated `startRecording()` to pass `guildId` and `userId`

### 3. Memory Optimization

**Before (per segment):**
```
AudioSegment.bufferChunks: Buffer[] // Kept in memory until recording stops
```

**After (per segment):**
```typescript
// During segment recording:
AudioSegment.bufferChunks: Buffer[]  // Only current segment

// After segment completes:
const bufferCopy = [...bufferChunks];  // Copy for upload
prevSegment.bufferChunks = [];  // CLEAR IMMEDIATELY

// After upload:
session.uploadedSegments.push({
  blobUrl, fileName, fileSize, ...metadata
});  // Only metadata in memory
```

### 4. Memory Monitoring

Added periodic logging (every 30 seconds) during recording:

```typescript
logger.info(`Memory usage update`, {
  heapUsed: "150MB",
  heapTotal: "200MB",
  rss: "300MB",
  sessionBuffers: "5MB",  // Only current segments
  uploadedSegments: 42,  // Already uploaded
  activeUsers: 4
});
```

### 5. Fallback Safety

The implementation gracefully falls back to batch export if:
- API `/init-live` fails â†’ continues recording, batch upload at end
- Segment upload fails â†’ keeps in memory, batch upload at end
- Finalize fails â†’ exports to disk, uses old batch upload flow

## Flow Comparison

### Old Flow (Batch Upload)
```
Start recording
  â†“
Capture all audio in memory (hours of data)
  â†“
Stop recording
  â†“
Write all segments to disk
  â†“
Upload all files to blob storage
  â†“
Complete
```
**Memory usage**: Linear growth, peaks at end

### New Flow (Streaming Upload)
```
Start recording â†’ POST /init-live
  â†“
Segment 0 completes â†’ Upload â†’ Clear from memory
  â†“
Segment 1 completes â†’ Upload â†’ Clear from memory
  â†“
... (repeat for hours) ...
  â†“
Segment N completes â†’ Upload â†’ Clear from memory
  â†“
Stop recording â†’ POST /finalize
```
**Memory usage**: Constant (~5-10MB)

## Testing Checklist

### Unit Tests
- [ ] `RecordingUploadService.initLiveRecording()` handles API errors
- [ ] `RecordingUploadService.uploadSegmentImmediately()` uploads correctly
- [ ] `RecordingUploadService.finalizeRecording()` sends all segments
- [ ] Memory is cleared after segment upload

### Integration Tests
- [ ] Start recording â†’ verify `init-live` called
- [ ] Segment completes â†’ verify uploaded immediately
- [ ] Stop recording â†’ verify all segments in `finalize` request
- [ ] Upload failure â†’ verify fallback to batch export
- [ ] API failure â†’ verify graceful degradation

### End-to-End Tests
- [ ] Record 5-minute session with 2 users
- [ ] Monitor memory usage (should stay flat)
- [ ] Verify all segments uploaded to blob storage
- [ ] Verify recording finalized in database
- [ ] Verify temp files cleaned up

### Load Tests
- [ ] Record 3-hour session with 4 users
- [ ] Monitor Railway memory metrics (should not exceed limit)
- [ ] Verify ~4,600 segment uploads complete successfully
- [ ] Verify rate limiting not exceeded (~25 API calls/minute)

## Configuration

No new environment variables required. Streaming uploads are automatic when API endpoints are available.

**Existing config used:**
- `RECORDING_MIN_SEGMENT_DURATION` - Minimum segment length
- `RECORDING_SILENCE_THRESHOLD` - Silence gap detection
- `RECORDING_KEEP_LOCAL_AFTER_UPLOAD` - Cleanup behavior (not used for streaming)

## Performance Impact

### API Calls
**Old**: 3 calls per recording (init + complete + upload proxy)
**New**: 1 + N + 1 calls (init-live + N segments + finalize)

For 3-hour recording with 4 users:
- ~4,600 segment upload requests
- ~25 requests/minute average
- Well within API limits

### Bandwidth
**Same total bandwidth** (files still uploaded), but:
- **Old**: Large spike at end (upload all at once)
- **New**: Distributed over entire recording (smoother)

### Disk Usage
**Old**: Ephemeral disk fills up during recording
**New**: Temp files deleted immediately after upload (~constant disk usage)

### Memory Usage
**Old**: Linear growth, peaks at 8GB+ for long recordings
**New**: Constant ~5-10MB regardless of duration âœ…

## Migration Notes

### Backward Compatibility
The old batch upload flow is **still supported** and used as fallback:
- If `/init-live` endpoint doesn't exist â†’ use batch upload
- If segment upload fails â†’ collect in memory, batch upload at end
- If `finalize` fails â†’ export to disk, use old flow

### Rollout Strategy
1. Deploy bot changes **first** (API endpoints not yet available)
   - Bot will log "init-live failed", use batch upload (current behavior)
2. Deploy API changes with new endpoints
   - Bot automatically detects and uses streaming uploads
3. Monitor memory usage metrics
   - Should see immediate improvement

## Known Limitations

1. **Upload failures**: If many segments fail to upload, memory will grow
   - Mitigation: Logs errors, retries, falls back to batch

2. **Network latency**: Each segment upload adds ~100-500ms delay
   - Impact: Minimal, uploads happen asynchronously during next segment

3. **API rate limits**: High segment count could hit rate limits
   - Mitigation: Uploads are throttled by natural recording pace (~25/min)

4. **Temp disk space**: Railway /tmp may fill if uploads are slow
   - Mitigation: Files deleted immediately after upload

## Success Metrics

After deployment, expect to see:
- âœ… Constant memory usage during recording (not linear growth)
- âœ… No out-of-memory crashes on Railway
- âœ… Temp files cleaned up automatically
- âœ… All segments appear in blob storage
- âœ… Recordings finalized correctly

## Troubleshooting

### "Failed to init live recording via API"
- **Cause**: API endpoint not deployed yet
- **Impact**: None, falls back to batch upload
- **Fix**: Deploy API changes

### "Failed to upload segment X"
- **Cause**: Network issue or API error
- **Impact**: Segment kept in memory, uploaded at end
- **Fix**: Check API logs, verify blob storage credentials

### "No segments to export"
- **Cause**: All segments uploaded, but finalize failed
- **Impact**: Recording data in blob storage but not in database
- **Fix**: Manual recovery via blob storage listing

### Memory still growing
- **Cause**: Many upload failures causing fallback to memory
- **Impact**: Same as old behavior
- **Fix**: Investigate upload failures in logs

## Next Steps (Future Optimizations)

1. **Direct Blob Upload**: Use Vercel Blob client tokens instead of proxy
   - Eliminates API proxy overhead
   - See `RecordingUploadService.ts:127-146` TODO comment

2. **Parallel uploads**: Upload multiple segments concurrently
   - Currently uploads sequentially
   - Could reduce total upload time

3. **Compression**: Compress segments before upload
   - Trade CPU for bandwidth
   - Consider FLAC or Opus instead of WAV

4. **Incremental transcription**: Start transcribing while recording
   - Don't wait for full recording to finish
   - Provide real-time transcript updates

## Summary

**Before**: 3-hour recording â†’ 8GB RAM â†’ crash ðŸ’¥
**After**: 3-hour recording â†’ 10MB RAM â†’ success âœ…

The streaming upload architecture ensures constant memory usage regardless of recording duration, solving the Railway memory limit issue while maintaining backward compatibility with the existing system.
